{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo using MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from NeuralNet.nnet import NeuralNet\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./resources/train.csv')\n",
    "test_df = pd.read_csv('./resources/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = train_df.head(30000)\n",
    "validation = train_df.tail(12000)\n",
    "del train_df # free some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, label_list):\n",
    "    new_labels = []\n",
    "    for lab in labels:\n",
    "        y = np.zeros(len(label_list))\n",
    "        y[label_list.index(lab)] = 1\n",
    "        new_labels.append(y)\n",
    "    return (new_labels)\n",
    "def normalize_data(data):\n",
    "    labels = []\n",
    "    features = data.iloc[:, 1:] / 255\n",
    "    try:\n",
    "        labels = data['label']\n",
    "        print('label len =', len(labels))\n",
    "        sns.countplot(labels)\n",
    "        labels = one_hot_encode(labels, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return (features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features, t_labels = normalize_data(training)\n",
    "del training # free some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features, v_labels = normalize_data(validation)\n",
    "del validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = t_features.iloc[870].values\n",
    "example = example.reshape((28, 28))\n",
    "plt.imshow(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, topology):\n",
    "        \"\"\"\n",
    "        Specify the Network's architecture.\n",
    "        \n",
    "        Initializes weights and biases (He initialization) of the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "        Size of the input layer of the Network\n",
    "        topology : tuple\n",
    "        Tuple that contains the number of nodes per layer\n",
    "        (e.g. (256, 128) --> 2 layers of 256 and 128 nodes respectively)\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.topology = topology\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        prev_layer_size = input_size\n",
    "        for layer_size in topology:\n",
    "            self.weights.append(np.random.randn(layer_size, prev_layer_size) * np.sqrt(2 / prev_layer_size))\n",
    "            self.biases.append(np.zeros(layer_size))\n",
    "            prev_layer_size = layer_size\n",
    "    def softmax(self, vec):\n",
    "        \"\"\"Calculate the softmax of a vector.\"\"\"\n",
    "        ex_vec = np.exp(vec)\n",
    "        return (ex_vec / sum(ex_vec))\n",
    "    def forwardprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Forward propagation of the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy array\n",
    "        Current training example\n",
    "        y : numpy array\n",
    "        Current label example\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ops : list\n",
    "        Contains every vector calculated during the forward propagation from input to output (both included)\n",
    "        loss : int\n",
    "        Loss of the given the current training example\n",
    "        \"\"\"\n",
    "        ops = [x]\n",
    "        for i in range(len(self.topology)):\n",
    "            z = np.matmul(self.weights[i], ops[-1]) + self.biases[i]\n",
    "            if (i == len(self.topology) - 1):\n",
    "                a = self.softmax(z)\n",
    "            else:\n",
    "                a = np.where(z < 0, 0, z)\n",
    "            ops.append(z)\n",
    "            ops.append(a)\n",
    "        loss = -1 * np.log(np.dot(ops[-1], y))\n",
    "        return (ops, loss)\n",
    "    def backprop(self, l_rate, ops, y):\n",
    "        \"\"\"\n",
    "        Backpropagation of the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        l_rate : float\n",
    "        Learning rate\n",
    "        ops : list\n",
    "        Contains every vector calculated during the forward propagation\n",
    "        y : numpy array\n",
    "        Current label example\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "        Contains every gradient calculated during the backpropagation\n",
    "        \"\"\"\n",
    "        grads = []\n",
    "        grad_z = ops.pop() - y\n",
    "        ops.pop()\n",
    "        for i in range(len(self.topology)):\n",
    "            grads.append(np.outer(grad_z, ops.pop()))\n",
    "            grads.append(grad_z)\n",
    "            if (i < len(self.topology) - 1):\n",
    "                grad_prev_a = np.dot(self.weights[-i - 1].T, grad_z)\n",
    "                grad_actv_funct = np.where(ops.pop() > 0, 1, 0)\n",
    "                grad_z = grad_actv_funct * grad_prev_a\n",
    "        \n",
    "        # update weights and biases\n",
    "        ret_grads = grads.copy()\n",
    "        for i in range(len(self.topology)):\n",
    "            a = grads.pop()\n",
    "            b = grads.pop()\n",
    "            self.biases[i] -= l_rate * a\n",
    "            self.weights[i] -= l_rate * b\n",
    "        return (ret_grads)\n",
    "    def plot_stats(self, stats):\n",
    "        \"\"\"Plots training and validation's loss and accuracy.\"\"\"\n",
    "        df = pd.DataFrame(stats)\n",
    "        df_loss = df.iloc[:, :2]\n",
    "        df_acc = df.iloc[:, 2:4]\n",
    "        f, ax = plt.subplots(figsize=(20, 15))\n",
    "        plt.plot(df_loss, linewidth=4)\n",
    "        f.legend((\"Training\", \"Validation\"), fontsize=25)\n",
    "        plt.xticks(fontsize=25)\n",
    "        plt.yticks(fontsize=25)\n",
    "        plt.xlabel('Steps', fontsize=25)\n",
    "        plt.ylabel(\"Loss\", fontsize=25)\n",
    "        plt.title(\"Cross Entropy over whole dataset\", fontsize=40);\n",
    "        \n",
    "        f, ax = plt.subplots(figsize=(20, 15))\n",
    "        plt.plot(df_acc, linewidth=4)\n",
    "        f.legend((\"Training\", \"Validation\"), fontsize=25)\n",
    "        plt.xticks(fontsize=25)\n",
    "        plt.yticks(fontsize=25)\n",
    "        plt.xlabel('Steps', fontsize=25)\n",
    "        plt.ylabel(\"Accuracy\", fontsize=25)\n",
    "        plt.title(\"Accuracy over whole dataset\", fontsize=40);\n",
    "    def update_stats(self, t_features, t_labels, v_features, v_labels):\n",
    "        \"\"\"Saves new stats using the current weights and bias.\"\"\"\n",
    "        t_loss = 0\n",
    "        t_acc = 0\n",
    "        v_loss = 0\n",
    "        v_acc = 0\n",
    "        for x, y in zip(t_features, t_labels):\n",
    "            ops, loss = self.forwardprop(x, y)\n",
    "            t_loss += loss\n",
    "            ind = np.argmax(ops[-1])\n",
    "            t_acc += y[ind]\n",
    "        t_loss /= len(t_features)\n",
    "        t_acc /= len(t_features)\n",
    "        for x, y, in zip(v_features, v_labels):\n",
    "            ops, loss = self.forwardprop(x, y)\n",
    "            v_loss += loss\n",
    "            ind = np.argmax(ops[-1])\n",
    "            v_acc += y[ind]\n",
    "        v_loss /= len(v_features)\n",
    "        v_acc /= len(v_features)\n",
    "        print('training:\\n loss:', t_loss, 'accuracy:', t_acc)\n",
    "        print('validation:\\n loss:', v_loss, 'accuracy', t_acc)\n",
    "        print()\n",
    "        return ((t_loss, v_loss, t_acc, v_acc))\n",
    "    def next_batch(self, features, labels, batch_size):\n",
    "        \"\"\"Yield the next batch for training.\"\"\"\n",
    "        for i in range(0, len(features), batch_size):\n",
    "            yield features[:][i:i + batch_size], labels[i:i + batch_size]\n",
    "    def training(self, t_features, t_labels, v_features, v_labels, l_rate, epochs, batch_size, plot_steps):\n",
    "        \"\"\"\n",
    "        Function to train the Network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        t_features : numpy array\n",
    "        Training features\n",
    "        t_labels : numpy array\n",
    "        Training labels\n",
    "        v_features : numpy array\n",
    "        Validation features\n",
    "        v_labels : numpy array\n",
    "        Validation labels\n",
    "        l_rate : float\n",
    "        Learning rate\n",
    "        epochs : int\n",
    "        Epochs of training\n",
    "        batch_size : int\n",
    "        Batch size for training\n",
    "        plot_steps : int\n",
    "        The amount of steps to take before plotting\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List of tuples containing training and validation's loss and accuracy\n",
    "        \"\"\"\n",
    "        steps = 0\n",
    "        stats = []\n",
    "        for epoch_nb in range(epochs):\n",
    "            generator = self.next_batch(t_features, t_labels, batch_size)\n",
    "            for batch_x, batch_y in generator:\n",
    "                batch_loss = 0\n",
    "                for x, y in zip(batch_x, batch_y):\n",
    "                    ops, loss = self.forwardprop(x, y)\n",
    "                    batch_loss += loss\n",
    "                    if (steps % plot_steps == 0):\n",
    "                        stats.append(self.update_stats(t_features, t_labels, v_features, v_labels))\n",
    "                    steps += 1\n",
    "                batch_loss /= batch_size\n",
    "                self.backprop(l_rate, ops, y)\n",
    "        return (stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(input_size=784, topology=(512, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = t_features.values\n",
    "test_labels = t_labels\n",
    "val_features = v_features.values\n",
    "val_labels = v_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = nn.training(test_features, test_labels, val_features, val_labels, l_rate=0.01, epochs=5, batch_size=75, plot_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot_stats(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
