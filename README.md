# Home Made Neural Network
End-to-end customizable Neural Network built with Vanilla Python and Numpy.
### Currently Implemented
- Any network size and topology
- Only for classification tasks
- Gradient Descent
- ReLU activation function (softmax in the last)
- Batch Training
- Training and Validation examples (to check performance)
- Current Metrics - Cross entropy loss and accuracy
- Plotting
## How to run
1. Install the requirements and clone the repository:
```bash
pip install -r requirements.txt
git clone https://github.com/ThomasH88/home_made_NN.git
```
2. Download the MNIST Dataset from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data) and place it inside the ```resources/``` directory

3. Launch the demo with Jupyter Notebook
### Possible Future Implementations
- Different Optimizers (RMSprop, Adagrad, Adam, etc.)
- Different Activation functions
- Batch normalization
- Dropout
- Cool Visualizations
- A way to save/load weights
- Different Weight initializations
